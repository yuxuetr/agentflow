// AgentFlow Chat Example
// Migrated from PocketFlow cookbook/pocketflow-chat
// Tests: Interactive chat with conversation history, self-looping flow patterns

use agentflow_core::{AgentFlowError, AsyncFlow, AsyncNode, Result, SharedState};
use async_trait::async_trait;
use serde_json::Value;
use std::collections::HashMap;
use std::io::{self, Write};
use tokio::time::{sleep, Duration};

/// Mock LLM call for interactive chat
async fn call_mock_llm(messages: &[HashMap<String, String>]) -> String {
  // Simulate API call delay
  sleep(Duration::from_millis(150)).await;

  // Get the last user message
  let last_message = messages
    .last()
    .and_then(|msg| msg.get("content"))
    .map_or("Hello", |s| s.as_str());

  // Generate contextual responses based on conversation
  match last_message.to_lowercase().as_str() {
    msg if msg.contains("hello") || msg.contains("hi") => {
      "Hello! I'm your AgentFlow chat assistant. How can I help you today?".to_string()
    },
    msg if msg.contains("weather") => {
      "I'm a mock assistant, so I can't check real weather, but it's always sunny in the world of Rust! üåû".to_string()
    },
    msg if msg.contains("rust") => {
      "Rust is an amazing systems programming language! It provides memory safety without garbage collection. What would you like to know about it?".to_string()
    },
    msg if msg.contains("agentflow") => {
      "AgentFlow is a high-performance async framework for building intelligent agent workflows in Rust. It's inspired by PocketFlow but built for production scale!".to_string()
    },
    msg if msg.contains("joke") => {
      "Why do Rust developers never get lost? Because they always know where their ownership is! ü¶Ä".to_string()
    },
    msg if msg.contains("help") => {
      "I can chat with you about various topics! Try asking about Rust, AgentFlow, weather, or ask for a joke. Type 'exit' to end our conversation.".to_string()
    },
    msg if msg.contains("bye") || msg.contains("goodbye") => {
      "Goodbye! It was great chatting with you. Have a wonderful day! üëã".to_string()
    },
    _ => {
      // Generic response that acknowledges the conversation length
      let msg_count = messages.len();
      match msg_count {
        1..=2 => "That's interesting! Tell me more about what you're thinking.".to_string(),
        3..=5 => format!("I see we're having a good conversation (message #{})! What else would you like to discuss?", msg_count),
        _ => format!("We've been chatting for a while now ({} messages)! I'm enjoying our conversation. What's on your mind?", msg_count)
      }
    }
  }
}

/// Get user input from console (async wrapper)
async fn get_user_input() -> io::Result<String> {
  tokio::task::spawn_blocking(|| {
    print!("\nYou: ");
    io::stdout().flush().unwrap();

    let mut input = String::new();
    io::stdin().read_line(&mut input)?;
    Ok(input.trim().to_string())
  })
  .await
  .unwrap()
}

/// Chat Node - equivalent to PocketFlow's ChatNode
/// Tests conversation history management and self-looping flow patterns
struct ChatNode {
  node_id: String,
}

impl ChatNode {
  fn new() -> Self {
    Self {
      node_id: "chat_node".to_string(),
    }
  }
}

#[async_trait]
impl AsyncNode for ChatNode {
  async fn prep_async(&self, shared: &SharedState) -> Result<Value> {
    // Phase 1: Preparation - initialize messages and get user input

    // Initialize messages if this is the first run
    if shared.get("messages").is_none() {
      shared.insert("messages".to_string(), Value::Array(vec![]));
      println!("ü§ñ Welcome to AgentFlow Chat! Type 'exit' to end the conversation.");
    }

    // Get user input
    print!("\nüí¨ [PREP] Getting user input...");
    io::stdout().flush().unwrap();

    let user_input = match get_user_input().await {
      Ok(input) => input,
      Err(e) => {
        return Err(AgentFlowError::NodeExecutionFailed {
          message: format!("Failed to get user input: {}", e),
        })
      }
    };

    // Check if user wants to exit
    if user_input.to_lowercase() == "exit" {
      return Ok(Value::Null); // Signal to end conversation
    }

    // Get current messages
    let mut messages: Vec<HashMap<String, String>> = shared
      .get("messages")
      .and_then(|v| v.as_array().map(|arr| arr.clone()))
      .map(|arr| {
        arr
          .iter()
          .filter_map(|v| {
            if let (Some(role), Some(content)) = (v["role"].as_str(), v["content"].as_str()) {
              let mut msg = HashMap::new();
              msg.insert("role".to_string(), role.to_string());
              msg.insert("content".to_string(), content.to_string());
              Some(msg)
            } else {
              None
            }
          })
          .collect()
      })
      .unwrap_or_else(Vec::new);

    // Add user message to history
    let mut user_message = HashMap::new();
    user_message.insert("role".to_string(), "user".to_string());
    user_message.insert("content".to_string(), user_input);
    messages.push(user_message);

    println!("üîç [PREP] Conversation length: {} messages", messages.len());

    // Convert back to JSON format for processing
    let messages_json: Vec<Value> = messages
      .iter()
      .map(|msg| {
        serde_json::json!({
          "role": msg["role"],
          "content": msg["content"]
        })
      })
      .collect();

    Ok(serde_json::json!({
      "messages": messages_json,
      "current_messages": messages
    }))
  }

  async fn exec_async(&self, prep_result: Value) -> Result<Value> {
    // Phase 2: Execution - call LLM with conversation history
    if prep_result.is_null() {
      return Ok(Value::Null); // Signal to end
    }

    let messages: Vec<HashMap<String, String>> = prep_result["current_messages"]
      .as_array()
      .unwrap_or(&vec![])
      .iter()
      .filter_map(|v| {
        if let (Some(role), Some(content)) = (v["role"].as_str(), v["content"].as_str()) {
          let mut msg = HashMap::new();
          msg.insert("role".to_string(), role.to_string());
          msg.insert("content".to_string(), content.to_string());
          Some(msg)
        } else {
          None
        }
      })
      .collect();

    if messages.is_empty() {
      return Err(AgentFlowError::NodeExecutionFailed {
        message: "No messages to process".to_string(),
      });
    }

    println!("ü§ñ [EXEC] Calling LLM with {} messages...", messages.len());
    let response = call_mock_llm(&messages).await;

    println!("‚ö° [EXEC] Received LLM response");

    Ok(serde_json::json!({
      "response": response,
      "messages": prep_result["messages"]
    }))
  }

  async fn post_async(
    &self,
    shared: &SharedState,
    _prep: Value,
    exec: Value,
  ) -> Result<Option<String>> {
    // Phase 3: Post-processing - display response and update conversation history
    if exec.is_null() {
      println!("\nüëã Goodbye! Thanks for chatting with AgentFlow!");
      return Ok(None); // End the conversation
    }

    let response = exec["response"].as_str().unwrap_or("(No response)");
    let messages = exec["messages"].as_array().unwrap();

    // Display the assistant's response
    println!("\nü§ñ Assistant: {}", response);

    // Add assistant message to history
    let mut updated_messages = messages.clone();
    updated_messages.push(serde_json::json!({
      "role": "assistant",
      "content": response
    }));

    // Update shared state with conversation history
    shared.insert(
      "messages".to_string(),
      Value::Array(updated_messages.clone()),
    );
    shared.insert(
      "message_count".to_string(),
      Value::Number(updated_messages.len().into()),
    );

    println!(
      "üíæ [POST] Updated conversation history ({} messages)",
      updated_messages.len()
    );

    // Return "continue" to loop back for next user input
    Ok(Some("continue".to_string()))
  }

  fn get_node_id(&self) -> Option<String> {
    Some(self.node_id.clone())
  }
}

/// Create chat flow with self-looping pattern
fn create_chat_flow() -> AsyncFlow {
  let chat_node = Box::new(ChatNode::new());

  // In AgentFlow, we handle self-looping through the post_async return value
  // The flow will continue when post_async returns Some("continue")
  AsyncFlow::new(chat_node)
}

/// Main chat example function
async fn run_chat_example() -> Result<()> {
  println!("üöÄ AgentFlow Interactive Chat Example");
  println!("üìù Migrated from: PocketFlow cookbook/pocketflow-chat");
  println!("üéØ Testing: Interactive chat, conversation history, self-looping flows\n");

  // Create shared state for conversation
  let shared = SharedState::new();

  // Create and run the chat flow
  let chat_flow = create_chat_flow();

  println!("üîÑ Starting interactive chat session...");

  // The flow will run in a loop until the user types 'exit'
  match chat_flow.run_async(&shared).await {
    Ok(result) => {
      println!("\n‚úÖ Chat session completed successfully");
      println!("üìã Final result: {:?}", result);

      // Display conversation summary
      let message_count = shared
        .get("message_count")
        .and_then(|v| v.as_u64())
        .unwrap_or(0);

      println!("\nüéØ Chat Session Summary:");
      println!("Total Messages: {}", message_count);

      if message_count > 0 {
        println!("Conversation History:");
        if let Some(messages_value) = shared.get("messages") {
          if let Some(messages) = messages_value.as_array() {
            for (i, msg) in messages.iter().enumerate() {
              let role = msg["role"].as_str().unwrap_or("unknown");
              let content = msg["content"].as_str().unwrap_or("(empty)");
              let truncated = if content.len() > 50 {
                format!("{}...", &content[..47])
              } else {
                content.to_string()
              };
              println!("  {}: {} - {}", i + 1, role, truncated);
            }
          }
        }
      }

      println!("\n‚úÖ AgentFlow chat functionality verified!");
    }
    Err(e) => {
      println!("‚ùå Chat session failed: {}", e);
      return Err(e);
    }
  }

  Ok(())
}

/// Non-interactive demo for testing
async fn run_demo_conversation() -> Result<()> {
  println!("\nüé≠ Running automated demo conversation...");

  let shared = SharedState::new();

  // Simulate a conversation programmatically
  let demo_messages = vec![
    "Hello!",
    "Tell me about Rust",
    "What is AgentFlow?",
    "Can you tell me a joke?",
    "Goodbye!",
  ];

  // Initialize conversation
  shared.insert("messages".to_string(), Value::Array(vec![]));

  for (i, user_msg) in demo_messages.iter().enumerate() {
    println!("\n--- Demo Turn {} ---", i + 1);
    println!("üë§ User: {}", user_msg);

    // Get current messages
    let mut messages: Vec<HashMap<String, String>> = shared
      .get("messages")
      .and_then(|v| v.as_array().map(|arr| arr.clone()))
      .map(|arr| {
        arr
          .iter()
          .filter_map(|v| {
            if let (Some(role), Some(content)) = (v["role"].as_str(), v["content"].as_str()) {
              let mut msg = HashMap::new();
              msg.insert("role".to_string(), role.to_string());
              msg.insert("content".to_string(), content.to_string());
              Some(msg)
            } else {
              None
            }
          })
          .collect()
      })
      .unwrap_or_else(Vec::new);

    // Add user message
    let mut user_message = HashMap::new();
    user_message.insert("role".to_string(), "user".to_string());
    user_message.insert("content".to_string(), user_msg.to_string());
    messages.push(user_message);

    // Get LLM response
    let response = call_mock_llm(&messages).await;
    println!("ü§ñ Assistant: {}", response);

    // Add assistant message
    let mut assistant_message = HashMap::new();
    assistant_message.insert("role".to_string(), "assistant".to_string());
    assistant_message.insert("content".to_string(), response);
    messages.push(assistant_message);

    // Update shared state
    let messages_json: Vec<Value> = messages
      .iter()
      .map(|msg| {
        serde_json::json!({
          "role": msg["role"],
          "content": msg["content"]
        })
      })
      .collect();

    shared.insert("messages".to_string(), Value::Array(messages_json));
  }

  println!("\n‚úÖ Demo conversation completed!");
  Ok(())
}

/// Performance comparison with PocketFlow
async fn performance_comparison() {
  println!("\nüìä Chat Performance Comparison:");
  println!("PocketFlow (Python):");
  println!("  - Synchronous input/output");
  println!("  - Dict-based message history");
  println!("  - String-based flow routing");
  println!("  - Blocking I/O operations");
  println!();
  println!("AgentFlow (Rust):");
  println!("  - Async input/output with tokio");
  println!("  - Structured message history with JSON");
  println!("  - Type-safe flow control");
  println!("  - Non-blocking operations");
  println!();
  println!("Expected improvements:");
  println!("  - üöÄ Better resource utilization");
  println!("  - ‚ö° Non-blocking user interactions");
  println!("  - üíß Lower memory usage for conversation history");
  println!("  - üõ°Ô∏è Type-safe message handling");
  println!("  - üìä Built-in conversation analytics");
}

#[tokio::main]
async fn main() -> Result<()> {
  // Run automated demo first
  run_demo_conversation().await?;

  // Show performance comparison
  performance_comparison().await;

  // Prompt user for interactive session
  println!("\nüéÆ Would you like to try interactive chat? (y/n)");
  let mut input = String::new();
  if io::stdin().read_line(&mut input).is_ok() && input.trim().to_lowercase() == "y" {
    run_chat_example().await?;
  }

  println!("\nüéâ Chat migration completed successfully!");
  println!("üî¨ AgentFlow chat functionality verified:");
  println!("  ‚úÖ Interactive conversation flow");
  println!("  ‚úÖ Conversation history management");
  println!("  ‚úÖ Self-looping flow patterns");
  println!("  ‚úÖ Async user input handling");
  println!("  ‚úÖ Structured message storage");

  Ok(())
}

#[cfg(test)]
mod tests {
  use super::*;

  #[tokio::test]
  async fn test_mock_llm_responses() {
    let messages = vec![{
      let mut msg = HashMap::new();
      msg.insert("role".to_string(), "user".to_string());
      msg.insert("content".to_string(), "Hello".to_string());
      msg
    }];

    let response = call_mock_llm(&messages).await;
    assert!(response.to_lowercase().contains("hello"));
  }

  #[tokio::test]
  async fn test_conversation_context() {
    let messages = vec![
      {
        let mut msg = HashMap::new();
        msg.insert("role".to_string(), "user".to_string());
        msg.insert("content".to_string(), "Hi".to_string());
        msg
      },
      {
        let mut msg = HashMap::new();
        msg.insert("role".to_string(), "assistant".to_string());
        msg.insert("content".to_string(), "Hello!".to_string());
        msg
      },
      {
        let mut msg = HashMap::new();
        msg.insert("role".to_string(), "user".to_string());
        msg.insert("content".to_string(), "Tell me about Rust".to_string());
        msg
      },
    ];

    let response = call_mock_llm(&messages).await;
    assert!(response.to_lowercase().contains("rust"));
    assert!(messages.len() == 3); // Context preservation
  }

  #[tokio::test]
  async fn test_chat_node_with_mock_data() {
    let node = ChatNode::new();
    let shared = SharedState::new();

    // Initialize with some messages
    shared.insert(
      "messages".to_string(),
      Value::Array(vec![
        serde_json::json!({"role": "user", "content": "Hello"}),
      ]),
    );

    // Note: This test focuses on the logic that doesn't require user input
    // Full interactive testing would require more complex mock setups

    assert_eq!(node.get_node_id(), Some("chat_node".to_string()));
  }
}
