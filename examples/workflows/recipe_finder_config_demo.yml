# AgentFlow Recipe Finder - Configuration-First Demo
# Demonstrates the conversion from code-first to configuration-first approach
# Note: Currently uses mock LLM responses (Core's LlmNode limitation)
# In a production system, this would use real API calls
name: "Recipe Finder Config Demo"
version: "1.0.0"
description: "Demo of configuration-first workflow conversion from recipe_finder_real_llm.rs"

# Input parameters for the workflow
inputs:
  ingredient:
    type: string
    required: true
    description: "Main ingredient to find recipes for"

# Linear workflow demonstrating the conversion
workflow:
  type: sequential
  nodes:
    # Step 1: Recipe generation (simulated)
    - name: fetch_recipes
      type: llm
      config:
        model: "step-2-mini"
        prompt: |
          Generate 5 recipes for {{ inputs.ingredient }}:
          1. Grilled {{ inputs.ingredient }}
          2. Baked {{ inputs.ingredient }}
          3. {{ inputs.ingredient }} Stir Fry
          4. Roasted {{ inputs.ingredient }}
          5. {{ inputs.ingredient }} Curry
          
          Choose the best one for recommendation.
        system: "You are a recipe recommender. Respond with the best recipe name only."
        temperature: 0.7
        max_tokens: 100
      outputs:
        recipe_suggestion: response

    # Step 2: User approval simulation
    - name: user_approval
      type: llm
      config:
        model: "step-2-mini"
        prompt: |
          Suggested recipe: {{ fetch_recipes_response }}
          
          Would a typical user approve this {{ inputs.ingredient }} recipe?
          Consider if it sounds tasty and well-balanced.
          Respond with either APPROVED or REJECTED.
        system: "You are simulating user preferences for recipes. Generally approve good recipes."
        temperature: 0.3
        max_tokens: 10
      outputs:
        approval_result: response

    # Step 3: Final result generation
    - name: final_response
      type: llm
      config:
        model: "step-2-mini"
        prompt: |
          Recipe: {{ fetch_recipes_response }}
          Decision: {{ user_approval_response }}
          Ingredient: {{ inputs.ingredient }}
          
          Generate a friendly response about the outcome.
        system: "You are a helpful cooking assistant."
        temperature: 0.8
        max_tokens: 150
      outputs:
        final_message: response

# Final outputs
outputs:
  suggested_recipe:
    source: "fetch_recipes_response"
    description: "The recommended recipe"
  
  user_decision:
    source: "user_approval_response"
    description: "User approval decision"
  
  final_result:
    source: "final_response_response"
    description: "Final workflow message"

# Conversion notes
metadata:
  version: "1.0.0"
  conversion_source: "recipe_finder_real_llm.rs"
  original_features:
    - "Real StepFun API calls"
    - "JSON recipe parsing"
    - "Intelligent recipe selection"
    - "Retry logic with max attempts"
    - "Complex approval simulation"
  
  config_equivalent:
    - "Sequential LLM workflow steps"
    - "Template variable substitution"
    - "Simplified approval logic"
    - "Output mapping and collection"
  
  limitations:
    - "Mock responses instead of real API calls"
    - "No retry/loop logic in current config system"
    - "Simplified prompt structures"
  
  notes: |
    This demonstrates the structural conversion from code-first to configuration-first.
    
    Code-first advantages preserved:
    - Template substitution ({{ variable }})
    - Sequential step execution
    - State management between steps
    - Configurable parameters
    
    Code-first features requiring enhancement:
    - Real LLM API integration in ConfigWorkflowRunner
    - Conditional logic and retry loops
    - Complex error handling
    - Dynamic workflow control
    
    To enable real API calls, the Core's LlmNode would need enhancement
    to integrate with agentflow-llm's LLMClientBuilder like the code-first version.

# Real API integration example (for future enhancement)
future_enhancements:
  real_llm_integration: |
    # This would require Core's LlmNode to be enhanced with:
    workflow:
      - name: real_fetch_recipes
        type: llm
        model: "step-2-mini"
        api_config:
          provider: "stepfun"
          api_key_env: "STEPFUN_API_KEY"
          real_api: true  # Enable actual API calls
        prompt: |
          Generate exactly 5 different recipes using "{{ ingredient }}" as the main ingredient.
          Return ONLY a JSON array of recipe names.
        temperature: 0.8
        max_tokens: 200
        
  conditional_logic: |
    # Future enhancement for retry logic:
    workflow:
      - name: approval_check
        type: conditional
        condition: "{{ approval_result }} == 'REJECTED'"
        if_true: "retry_suggestion"
        if_false: "workflow_complete"
        max_iterations: 4