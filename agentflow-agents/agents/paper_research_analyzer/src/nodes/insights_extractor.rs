//! Key Insights Extraction Node - Extract structured metadata and insights

use agentflow_agents::{AsyncNode, SharedState, AgentFlowError, AgentFlow};
use async_trait::async_trait;
use serde_json::{json, Value};

pub struct InsightsNode {
  model: String,
}

impl InsightsNode {
  pub fn new(model: String) -> Self {
    Self { model }
  }

  /// Get model capacity for insights extraction (75% of full capacity)
  fn get_model_capacity_for_insights(&self) -> usize {
    let full_capacity = match self.model.as_str() {
      m if m.contains("qwen-turbo") || m.contains("qwen-plus-latest") || m.contains("qwen-long") => 800_000,
      m if m.contains("256k") => 200_000,
      m if m.contains("32k") => 80_000,
      m if m.contains("claude") => 180_000,
      m if m.contains("gpt-4o") => 120_000,
      _ => 30_000
    };
    (full_capacity as f64 * 0.75) as usize
  }
}

#[async_trait]
impl AsyncNode for InsightsNode {
  async fn prep_async(&self, shared: &SharedState) -> Result<Value, AgentFlowError> {
    let content = shared.get("pdf_content").ok_or_else(|| 
      AgentFlowError::AsyncExecutionError { message: "PDF content not available".to_string() })?;
    
    Ok(json!({
      "content": content,
      "model": self.model
    }))
  }

  async fn exec_async(&self, prep_result: Value) -> Result<Value, AgentFlowError> {
    let content = prep_result["content"].as_str().unwrap();
    
    // Calculate reasonable truncation based on model context window  
    let max_content_chars = self.get_model_capacity_for_insights();
    
    let truncated_content = if content.len() > max_content_chars {
      println!("âš ï¸  Content too long for insights extraction ({}), truncating to {} characters for model {}", 
               content.len(), max_content_chars, self.model);
      &content[..max_content_chars]
    } else {
      content
    };
    
    println!("ðŸ” Extracting key insights and metadata...");

    let insights_prompt = format!(r#"
åˆ†æžè¿™ç¯‡ç ”ç©¶è®ºæ–‡ï¼Œå¹¶æŒ‰ä»¥ä¸‹JSONæ ¼å¼æå–å…³é”®æ´žå¯Ÿï¼š

{{
  "title": "è®ºæ–‡ç¡®åˆ‡æ ‡é¢˜",
  "authors": ["ä½œè€…åˆ—è¡¨"],
  "publication_year": "å‘è¡¨å¹´ä»½ï¼ˆå¦‚æœ‰ï¼‰",
  "field_of_study": "ä¸»è¦ç ”ç©¶é¢†åŸŸ",
  "research_type": "ç†è®º/å®žè¯/å®žéªŒ/ç»¼è¿°/è¯„è®º",
  "methodology": ["ä½¿ç”¨çš„æ–¹æ³•åˆ—è¡¨"],
  "key_contributions": ["ä¸»è¦è´¡çŒ®"],
  "novel_concepts": ["å¼•å…¥çš„æ–°æ¦‚å¿µ"],
  "datasets_used": ["æåˆ°çš„æ•°æ®é›†"],
  "evaluation_metrics": ["ç”¨äºŽè¯„ä¼°çš„æŒ‡æ ‡"],
  "future_work": ["å»ºè®®çš„æœªæ¥ç ”ç©¶æ–¹å‘"],
  "citations_mentioned": "å‚è€ƒæ–‡çŒ®æ•°é‡",
  "research_gap": "å¡«è¡¥äº†ä»€ä¹ˆç©ºç™½",
  "impact_potential": "high/medium/low",
  "reproducibility": "high/medium/low/unclear"
}}

Research Paper Content:
{}
"#, truncated_content);

    let response = AgentFlow::model(&self.model)
      .prompt(&insights_prompt)
      .temperature(0.2)
      .max_tokens(1500)
      .execute()
      .await
      .map_err(|e| AgentFlowError::AsyncExecutionError { 
        message: format!("Insights extraction failed: {}", e) 
      })?;

    println!("âœ… Key insights extracted successfully");

    // Try to parse as JSON to validate structure
    let insights_json: Value = serde_json::from_str(&response)
      .unwrap_or_else(|_| json!({"raw_response": response}));

    Ok(json!({
      "insights": insights_json,
      "model_used": self.model
    }))
  }

  async fn post_async(&self, shared: &SharedState, _prep_result: Value, exec_result: Value) -> Result<Option<String>, AgentFlowError> {
    println!("ðŸ” InsightsNode: Storing insights in shared state");
    shared.insert("insights".to_string(), exec_result);
    
    // Determine next node
    if shared.get("has_mindmap").and_then(|v| v.as_bool()).unwrap_or(false) {
      Ok(Some("mind_mapper".to_string()))
    } else if shared.get("has_translation").and_then(|v| v.as_bool()).unwrap_or(false) {
      Ok(Some("translator".to_string()))
    } else {
      Ok(Some("compiler".to_string()))
    }
  }

  fn get_node_id(&self) -> Option<String> {
    Some("insights_extractor".to_string())
  }
}