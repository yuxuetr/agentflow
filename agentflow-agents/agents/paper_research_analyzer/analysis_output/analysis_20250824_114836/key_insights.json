{
  "raw_response": "```json\n{\n  \"title\": \"Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding\",\n  \"authors\": [\"StepFun Inc.\"],\n  \"publication_year\": 2025,\n  \"field_of_study\": \"Large Language Models (LLMs) and Hardware-Aware Model-System Co-design\",\n  \"research_type\": \"Experimental\",\n  \"methodology\": [\"Model-system co-design\", \"Attention-FFN Disaggregation (AFD)\", \"Multi-Matrix Factorization Attention (MFA)\", \"Hardware-aware optimization\", \"Theoretical cost analysis\"],\n  \"key_contributions\": [\"Introduces Step-3, a 321B-parameter VLM with hardware-aware model-system co-design optimized for minimizing decoding costs.\", \"Proposes Multi-Matrix Factorization Attention (MFA) to reduce KV cache size and computation while maintaining attention expressiveness.\", \"Introduces Attention-FFN Disaggregation (AFD) to decouple attention and FFN layers into specialized subsystems for better efficiency.\", \"Demonstrates that Step-3 significantly reduces decoding costs compared to models like DeepSeek-V3 and Qwen3 MoE 235B, especially at longer context lengths.\"],\n  \"novel_concepts\": [\"Multi-Matrix Factorization Attention (MFA)\", \"Attention-FFN Disaggregation (AFD)\"],\n  \"datasets_used\": [],\n  \"evaluation_metrics\": [\"Theoretical decoding cost in USD\", \"Decoding throughput (tokens per second per GPU)\", \"KV cache size\", \"FFN computation\", \"Memory access\"],\n  \"future_work\": [\"Explore new attention variants to continue pushing the Pareto frontier of model volume and system costs.\", \"Investigate novel high bandwidth domain designs to mitigate the sparsity limitations of MoE FFN for efficient decoding.\"],\n  \"citations_mentioned\": 33,\n  \"research_gap\": \"Previous models faced low hardware efficiency during decoding, particularly for long-context reasoning tasks. There was a lack of effective strategies to balance parameter count, hardware efficiency, and decoding costs.\",\n  \"impact_potential\": \"high\",\n  \"reproducibility\": \"high\"\n}\n```"
}