# RAG System - Retrieval-Augmented Generation
name: "RAG Document Q&A System"
version: "1.0.0"
description: "A complete RAG system for document indexing and question answering using OpenAI models"
author: "AgentFlow Team"

metadata:
  created: "2024-01-15T00:00:00Z"
  tags: ["openai", "rag", "retrieval", "embeddings", "question-answering"]
  category: "advanced"

config:
  timeout: "20m"
  max_retries: 3
  output_format: "json"
  log_level: "info"

inputs:
  documents:
    type: "array"
    required: true
    description: "List of documents to index for retrieval"
    example: [
      "The Earth is the third planet from the Sun and the only known planet to harbor life.",
      "Machine learning is a subset of artificial intelligence that focuses on algorithms.",
      "The Great Wall of China is a series of fortifications built across northern China."
    ]
  
  query:
    type: "string"
    required: true
    description: "The question to ask about the documents"
    example: "What do you know about machine learning?"
  
  chunk_size:
    type: "number"
    required: false
    default: 500
    description: "Maximum characters per chunk when splitting documents"
  
  chunk_overlap:
    type: "number"
    required: false
    default: 50
    description: "Number of characters to overlap between chunks"
  
  top_k_results:
    type: "number"
    required: false
    default: 3
    description: "Number of most relevant chunks to retrieve"
  
  model:
    type: "string"
    required: false
    default: "gpt-4o"
    enum: ["gpt-4o", "gpt-4", "gpt-3.5-turbo", "gpt-4o-mini"]
    description: "OpenAI model to use for answer generation"

  embedding_model:
    type: "string"
    required: false
    default: "text-embedding-3-small"
    enum: ["text-embedding-3-small", "text-embedding-3-large", "text-embedding-ada-002"]
    description: "OpenAI embedding model to use"

  temperature:
    type: "number"
    required: false
    default: 0.1
    description: "Temperature for answer generation"

environment:
  OPENAI_API_KEY: "required"

workflow:
  type: "sequential"
  nodes:
    # Step 1: Chunk documents
    - name: "chunk_documents"
      type: "template"
      description: "Split documents into chunks for better retrieval"
      config:
        template: |
          [
            {% for doc in inputs.documents %}
              {% set chunks = doc | chunk_text(inputs.chunk_size, inputs.chunk_overlap) %}
              {% for chunk in chunks %}
                "{{ chunk | replace('"', '\\"') }}"{% if not (loop.last and loop.outer.last) %},{% endif %}
              {% endfor %}
            {% endfor %}
          ]
      outputs:
        chunks: "$.rendered"

    # Step 2: Generate embeddings for all chunks
    - name: "embed_chunks"
      type: "batch"
      description: "Generate embeddings for document chunks"
      batch_input: "{{ chunk_documents.chunks | fromjson }}"
      config:
        model: "{{ inputs.embedding_model }}"
        input: "{{ batch_item }}"
        timeout: "2m"
      outputs:
        embedding: "$.data[0].embedding"
        text: "{{ batch_item }}"

    # Step 3: Generate embedding for the query
    - name: "embed_query"
      type: "http"
      description: "Generate embedding for the user query"
      config:
        method: "POST"
        url: "https://api.openai.com/v1/embeddings"
        headers:
          Authorization: "Bearer {{ env.OPENAI_API_KEY }}"
          Content-Type: "application/json"
        body: |
          {
            "input": "{{ inputs.query }}",
            "model": "{{ inputs.embedding_model }}"
          }
        timeout: "2m"
      outputs:
        query_embedding: "$.data[0].embedding"

    # Step 4: Calculate similarities and find most relevant chunks
    - name: "find_relevant_chunks"
      type: "template"
      description: "Calculate cosine similarity and find top relevant chunks"
      depends_on: ["embed_chunks", "embed_query"]
      config:
        template: |
          {% set query_emb = embed_query.query_embedding %}
          {% set similarities = [] %}
          {% for chunk_result in embed_chunks.results %}
            {% set chunk_emb = chunk_result.embedding %}
            {% set similarity = query_emb | cosine_similarity(chunk_emb) %}
            {% set _ = similarities.append({
              "text": chunk_result.text,
              "similarity": similarity
            }) %}
          {% endfor %}
          {% set top_chunks = similarities | sort(attribute='similarity', reverse=true) | list %}
          {{ top_chunks[:inputs.top_k_results] | tojson }}
      outputs:
        relevant_chunks: "$.rendered"

    # Step 5: Generate answer using retrieved context
    - name: "generate_answer"
      type: "llm"
      description: "Generate answer using OpenAI with retrieved context"
      depends_on: ["find_relevant_chunks"]
      config:
        model: "{{ inputs.model }}"
        temperature: "{{ inputs.temperature }}"
        max_tokens: 1000
        system: "You are a helpful assistant that answers questions based on provided context. Use only the information given in the context to answer questions."
        prompt: |
          Context:
          {% for chunk in find_relevant_chunks.relevant_chunks | fromjson %}
          {{ loop.index }}. {{ chunk.text }}
          
          {% endfor %}

          Question: {{ inputs.query }}

          Please answer the question based only on the context provided above. If the context doesn't contain enough information to answer the question, say so.

          Answer:
        timeout: "3m"
      outputs:
        answer: "$.response"

    # Step 6: Create detailed response with sources
    - name: "create_response"
      type: "template"
      description: "Create detailed response with source information"
      depends_on: ["generate_answer", "find_relevant_chunks"]
      config:
        template: |
          {
            "question": "{{ inputs.query }}",
            "answer": "{{ generate_answer.answer }}",
            "sources": {{ find_relevant_chunks.relevant_chunks | fromjson | tojson }},
            "model_used": "{{ inputs.model }}",
            "embedding_model_used": "{{ inputs.embedding_model }}",
            "total_chunks_processed": {{ embed_chunks.results | length }},
            "top_k_retrieved": {{ inputs.top_k_results }}
          }
      outputs:
        detailed_response: "$.rendered"

outputs:
  answer:
    source: "{{ generate_answer.answer }}"
    format: "text"
    file: "output/answer.txt"
  
  detailed_response:
    source: "{{ create_response.detailed_response | fromjson }}"
    format: "json"
    file: "output/detailed_response.json"
  
  relevant_sources:
    source: "{{ find_relevant_chunks.relevant_chunks | fromjson }}"
    format: "json"
    file: "output/relevant_sources.json"
  
  execution_report:
    source: "$"
    format: "json"
    file: "output/execution_report.json"
    include:
      - execution_time
      - model_used
      - embedding_model_used
      - chunks_processed
      - retrieval_accuracy