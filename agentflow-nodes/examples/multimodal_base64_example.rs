//! Image Understanding with Automatic Local File Conversion Example
//! 
//! This example demonstrates using the ImageUnderstandNode for multimodal image analysis,
//! which automatically handles local image file conversion to base64 data URLs.

use agentflow_core::{AsyncNode, SharedState};
use agentflow_nodes::ImageUnderstandNode;
use agentflow_nodes::nodes::image_understand::VisionResponseFormat;
use serde_json::Value;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
  println!("ğŸš€ Image Understanding with Automatic Local File Conversion");
  println!("============================================================\n");

  // Create shared state
  let shared = SharedState::new();
  
  // Use local image path directly - ImageUnderstandNode will automatically convert to base64
  let image_path = "../assets/AgentFlow-crates.jpeg";
  
  if !std::path::Path::new(image_path).exists() {
    println!("âŒ Error: Image not found at {}", image_path);
    println!("   Please ensure the AgentFlow-crates.jpeg file exists.");
    return Ok(());
  }

  println!("ğŸ“ Using local image path directly: {}", image_path);
  println!("   ğŸ¤– ImageUnderstandNode will automatically convert to base64 internally\n");
  
  // Add context variables to shared state
  shared.insert("analysis_focus".to_string(), Value::String("crate relationships and system architecture".to_string()));
  shared.insert("project_context".to_string(), Value::String("AgentFlow workflow orchestration platform".to_string()));

  // 1. Architecture Analysis with Automatic Local Image Conversion
  println!("ğŸ—ï¸ Step 1: Architecture Analysis with Automatic Image Conversion");
  let architecture_analyzer = ImageUnderstandNode::image_analyzer("arch_analyzer", "step-1o-turbo-vision", image_path)
    .with_system_message("You are an expert software architect. Provide detailed analysis of system architecture diagrams.")
    .with_text_prompt("Analyze this AgentFlow architecture diagram. Focus on {{analysis_focus}}.")
    .with_input_keys(vec!["analysis_focus".to_string()])
    .with_temperature(0.3)
    .with_max_tokens(600)
    .with_response_format(VisionResponseFormat::Markdown);

  match architecture_analyzer.run_async(&shared).await {
    Ok(_) => {
      if let Some(analysis) = shared.get("arch_analyzer_output") {
        println!("âœ… Architecture Analysis:");
        let analysis_text = analysis.as_str().unwrap_or("Could not parse response");
        if analysis_text.len() > 1000 {
          println!("{}...\n[Truncated for display]\n", &analysis_text[..1000]);
        } else {
          println!("{}\n", analysis_text);
        }
      }
    }
    Err(e) => {
      println!("âŒ Architecture analysis failed: {}", e);
      println!("ğŸ’¡ This might be due to:");
      println!("   â€¢ Missing StepFun API key or incorrect model configuration");
      println!("   â€¢ Local image file access issues or file not found");
      println!("   â€¢ Image too large for the vision model API");
      println!("   â€¢ API rate limits or quota exceeded\n");
    }
  }

  // 2. Technical Documentation Generation  
  println!("ğŸ“– Step 2: Technical Documentation Generation");
  let doc_generator = ImageUnderstandNode::new("docs_generator", "step-1o-turbo-vision", 
    "Generate comprehensive technical documentation explaining the system components and their interactions in this {{project_context}} diagram.", 
    image_path)
    .with_system_message("You are a technical writer specializing in software architecture documentation.")
    .with_input_keys(vec!["project_context".to_string()])
    .with_temperature(0.2)
    .with_max_tokens(800)
    .with_response_format(VisionResponseFormat::Markdown);

  match doc_generator.run_async(&shared).await {
    Ok(_) => {
      if let Some(docs) = shared.get("docs_generator_output") {
        println!("âœ… Generated Documentation:");
        let docs_text = docs.as_str().unwrap_or("Could not parse response");
        if docs_text.len() > 1200 {
          println!("{}...\n[Truncated for display]\n", &docs_text[..1200]);
        } else {
          println!("{}\n", docs_text);
        }
      }
    }
    Err(e) => {
      println!("âŒ Documentation generation failed: {}\n", e);
    }
  }

  // 3. Alternative: Test with a remote image URL
  println!("ğŸŒ Step 3: Remote Image Analysis Test");
  println!("   Testing with a remote image URL to verify URL handling...");
  
  let remote_image_url = "https://yuxuetr.com/assets/ideal-img/about-dark.6816e71.1920.jpg";
  let online_test_node = ImageUnderstandNode::image_describer("online_test", "step-1o-turbo-vision", remote_image_url)
    .with_text_prompt("What do you see in this image? Provide a brief description.")
    .with_temperature(0.4)
    .with_max_tokens(200);

  match online_test_node.run_async(&shared).await {
    Ok(_) => {
      if let Some(result) = shared.get("online_test_output") {
        println!("âœ… Online Image Test Result:");
        println!("{}\n", result.as_str().unwrap_or("Could not parse response"));
      }
    }
    Err(e) => {
      println!("âŒ Online image test also failed: {}", e);
      println!("ğŸ’¡ This suggests a configuration or API issue rather than image format.\n");
    }
  }

  // 4. Image File Information  
  println!("ğŸ“Š Image File Analysis:");
  println!("   â€¢ File path: {}", image_path);
  println!("   â€¢ Format: Local file (automatically converted to base64 internally)");
  
  if let Ok(metadata) = std::fs::metadata(image_path) {
    println!("   â€¢ File size: {} bytes ({:.1} KB)", metadata.len(), metadata.len() as f64 / 1024.0);
    
    // Estimate base64 size (roughly 4/3 of original size)
    let estimated_base64_size = (metadata.len() as f64 * 4.0 / 3.0) as usize;
    println!("   â€¢ Estimated base64 size: ~{} characters ({:.1} KB)", estimated_base64_size, estimated_base64_size as f64 / 1024.0);
    
    if estimated_base64_size > 1_000_000 { // > 1MB base64
      println!("   âš ï¸  Warning: Image is quite large. Some APIs have size limits.");
    } else {
      println!("   âœ… Image size is reasonable for most multimodal APIs.");
    }
  }

  println!("\nğŸ Image understanding example completed!");
  println!("\nğŸ’¡ Key Learning Points:");
  println!("   â€¢ ImageUnderstandNode automatically handles both local files and remote URLs");
  println!("   â€¢ Local files are converted to base64 data URLs internally"); 
  println!("   â€¢ HTTP/HTTPS URLs are used directly without conversion");
  println!("   â€¢ Purpose-built API for vision tasks with optimized defaults");
  println!("   â€¢ Template support for dynamic prompts with shared state");
  println!("   â€¢ Specialized constructors (image_analyzer, image_describer, etc.)");
  
  println!("\nğŸ”§ Architecture Benefits:");
  println!("   â€¢ Clear separation: ImageUnderstandNode for vision, LLMNode for text");
  println!("   â€¢ Vision-optimized parameters and response formats");
  println!("   â€¢ Better error handling for image-specific issues");
  println!("   â€¢ Multi-image support for comparison and analysis workflows");
  
  println!("\nğŸš€ Usage Recommendations:");
  println!("   â€¢ Use ImageUnderstandNode for image analysis tasks");
  println!("   â€¢ Use LLMNode for text-only language model interactions");
  println!("   â€¢ Combine both in complex workflows that need text + vision");
  println!("   â€¢ Leverage specialized constructors for common vision patterns");

  Ok(())
}